{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c6c3ce",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "\n",
    "ROOT_DIR = \"./plot_data/experiment-1/tables/\"\n",
    "RESULTS_DIR = \"./plot_data/experiment-1/results/\"\n",
    "\n",
    "# meta_features = pd.read_csv(ROOT_DIR + 'meta_features_table.csv', index_col=0) # MinMax Scaled\n",
    "meta_features = pd.read_csv(ROOT_DIR + 'meta_features_table_alt.csv', index_col=0) # MinMax Scaled\n",
    "\n",
    "reg_quantifiers_eval = pd.read_csv(ROOT_DIR + 'reg_quantifiers_evaluation_table.csv', index_col=0)\n",
    "knn_quantifiers_eval = pd.read_csv(ROOT_DIR + 'knn_quantifiers_evaluation_table.csv', index_col=0)\n",
    "\n",
    "reg_quantifiers_eval_agg = pd.read_csv(ROOT_DIR + 'reg_quantifiers_evaluation_table_agg.csv', index_col=[0,1])\n",
    "knn_quantifiers_eval_agg = pd.read_csv(ROOT_DIR + 'knn_quantifiers_evaluation_table_agg.csv', index_col=[0,1])\n",
    "\n",
    "arr_table = pd.read_csv(ROOT_DIR + 'arr_table.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec90bd",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regressor(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table):\n",
    "    meta_features_table = meta_features_table\n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table\n",
    "    evaluation_table = evaluation_table\n",
    "\n",
    "    model_dict = {}\n",
    "    \n",
    "    X_train = meta_features_table.values\n",
    "    y_train = None\n",
    "    for quantifier in evaluation_table.index.levels[0].tolist():\n",
    "        y_train = evaluation_table.loc[quantifier]['abs_error'].values\n",
    "        model_dict[quantifier] = clone(model)\n",
    "        model_dict[quantifier].fit(X_train, y_train)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "# Evaluate Quantifier Recommender with Leave-One-Out\n",
    "def loo_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, recommender_eval_path: str = None):\n",
    "    aux_recommender_evaluation_table = pd.DataFrame(columns=[\"predicted_error\", \"true_error\"], index=evaluation_table.index)\n",
    "    for quantifier, recommender in model_dict.items():\n",
    "        # recommender_ = clone(recommender)\n",
    "        for dataset in evaluation_table.index.levels[1]:\n",
    "            recommender_ = clone(recommender)\n",
    "            X_test = meta_features_table.loc[dataset].values\n",
    "            X_test = np.array(X_test).reshape(1, -1)\n",
    "            y_test = evaluation_table.loc[quantifier, dataset]['abs_error']\n",
    "\n",
    "            X_train = meta_features_table.drop(index=dataset).values\n",
    "            y_train = evaluation_table.loc[quantifier].drop(index=dataset)['abs_error'].values\n",
    "\n",
    "            recommender_.fit(X_train, y_train)\n",
    "            predicted_error = recommender_.predict(X_test)[0]\n",
    "\n",
    "            aux_recommender_evaluation_table.loc[(quantifier, dataset)] = [predicted_error, y_test]\n",
    "    \n",
    "    datasets = aux_recommender_evaluation_table.index.get_level_values('dataset').unique()\n",
    "    recommender_evaluation_table = pd.DataFrame(columns=[\"predicted_ranking\", \"predicted_ranking_weights\", \"predicted_ranking_mae\",\n",
    "                                                            \"true_ranking\", \"true_ranking_weights\", \"true_ranking_mae\"], index=datasets)\n",
    "    for dataset in datasets:\n",
    "        filtered_result = aux_recommender_evaluation_table.xs(dataset, level='dataset')\n",
    "        \n",
    "        predicted_ranking = filtered_result.sort_values(by='predicted_error').index.tolist()\n",
    "        predicted_ranking_mae = [filtered_result.loc[quantifier, 'predicted_error'] for quantifier in predicted_ranking]\n",
    "\n",
    "        errors = np.array(predicted_ranking_mae)\n",
    "        denominator = np.sum(1/errors)\n",
    "        predicted_ranking_weights = (1/errors)/denominator\n",
    "\n",
    "        true_ranking = filtered_result.sort_values(by='true_error').index.tolist()\n",
    "        true_ranking_mae = [filtered_result.loc[quantifier, 'true_error'] for quantifier in true_ranking]\n",
    "\n",
    "        errors = np.array(true_ranking_mae)\n",
    "        if np.any(errors == 0):\n",
    "            errors = np.array([1e-6 if x == 0 else x for x in errors])\n",
    "        denominator = np.sum(1/errors)\n",
    "        true_ranking_weights = (1/errors)/denominator\n",
    "\n",
    "        recommender_evaluation_table.loc[dataset] = [predicted_ranking, predicted_ranking_weights, predicted_ranking_mae,\n",
    "                                                        true_ranking, true_ranking_weights, true_ranking_mae]\n",
    "        \n",
    "    if not recommender_eval_path is None:\n",
    "        recommender_evaluation_table.to_csv(recommender_eval_path)\n",
    "    \n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table.copy(deep=True)\n",
    "    not_agg_evaluation_table.sort_values(by=['quantifier', 'dataset'], inplace=True)\n",
    "    not_agg_evaluation_table.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return recommender_evaluation_table, not_agg_evaluation_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Evaluate Quantifier Recommender with Leave-One-Out\n",
    "def loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, recommender_eval_path: str = None):\n",
    "    aux_recommender_evaluation_table = pd.DataFrame(columns=[\"predicted_error\", \"true_error\"], index=evaluation_table.index)\n",
    "    for quantifier, recommender in model_dict.items():\n",
    "        # recommender_ = clone(recommender)\n",
    "        for dataset in evaluation_table.index.levels[1]:\n",
    "            recommender_ = clone(recommender)\n",
    "            X_test = meta_features_table.loc[dataset].values\n",
    "            X_test = np.array(X_test).reshape(1, -1)\n",
    "            y_test = evaluation_table.loc[quantifier, dataset]['abs_error']\n",
    "\n",
    "            X_train = meta_features_table.drop(index=dataset).values\n",
    "            y_train = evaluation_table.loc[quantifier].drop(index=dataset)['abs_error'].values\n",
    "\n",
    "            # nested tuning (inner CV) dentro do fold LOO (outer)\n",
    "            inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            # gs = GridSearchCV(\n",
    "            #     estimator=recommender_,\n",
    "            #     param_grid=param_grid,   # chaves: \"C\", \"epsilon\", \"gamma\", \"kernel\", etc.\n",
    "            #     scoring=\"neg_mean_absolute_error\",\n",
    "            #     cv=inner_cv,\n",
    "            #     n_jobs=-1,\n",
    "            #     refit=True,\n",
    "            #     verbose=0\n",
    "            # )\n",
    "            gs = BayesSearchCV(\n",
    "                estimator=recommender_,\n",
    "                search_spaces=param_grid,\n",
    "                n_iter=25,\n",
    "                scoring=\"neg_mean_absolute_error\",\n",
    "                cv=inner_cv,\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "            gs.fit(X_train, y_train)\n",
    "            \n",
    "            predicted_error = gs.predict(X_test)[0]\n",
    "\n",
    "            aux_recommender_evaluation_table.loc[(quantifier, dataset)] = [predicted_error, y_test]\n",
    "\n",
    "            print(f\"Finished {dataset}\")\n",
    "\n",
    "        print(f\"Finished {quantifier}\")\n",
    "\n",
    "    datasets = aux_recommender_evaluation_table.index.get_level_values('dataset').unique()\n",
    "    recommender_evaluation_table = pd.DataFrame(columns=[\"predicted_ranking\", \"predicted_ranking_weights\", \"predicted_ranking_mae\",\n",
    "                                                            \"true_ranking\", \"true_ranking_weights\", \"true_ranking_mae\"], index=datasets)\n",
    "    for dataset in datasets:\n",
    "        filtered_result = aux_recommender_evaluation_table.xs(dataset, level='dataset')\n",
    "        \n",
    "        predicted_ranking = filtered_result.sort_values(by='predicted_error').index.tolist()\n",
    "        predicted_ranking_mae = [filtered_result.loc[quantifier, 'predicted_error'] for quantifier in predicted_ranking]\n",
    "\n",
    "        errors = np.array(predicted_ranking_mae)\n",
    "        denominator = np.sum(1/errors)\n",
    "        predicted_ranking_weights = (1/errors)/denominator\n",
    "\n",
    "        true_ranking = filtered_result.sort_values(by='true_error').index.tolist()\n",
    "        true_ranking_mae = [filtered_result.loc[quantifier, 'true_error'] for quantifier in true_ranking]\n",
    "\n",
    "        errors = np.array(true_ranking_mae)\n",
    "        if np.any(errors == 0):\n",
    "            errors = np.array([1e-6 if x == 0 else x for x in errors])\n",
    "        denominator = np.sum(1/errors)\n",
    "        true_ranking_weights = (1/errors)/denominator\n",
    "\n",
    "        recommender_evaluation_table.loc[dataset] = [predicted_ranking, predicted_ranking_weights, predicted_ranking_mae,\n",
    "                                                        true_ranking, true_ranking_weights, true_ranking_mae]\n",
    "        \n",
    "    if not recommender_eval_path is None:\n",
    "        recommender_evaluation_table.to_csv(recommender_eval_path)\n",
    "    \n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table.copy(deep=True)\n",
    "    not_agg_evaluation_table.sort_values(by=['quantifier', 'dataset'], inplace=True)\n",
    "    not_agg_evaluation_table.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return recommender_evaluation_table, not_agg_evaluation_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def fit_knn(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table):\n",
    "    meta_features_table = meta_features_table\n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table\n",
    "    evaluation_table = evaluation_table\n",
    "    \n",
    "    eval_table = evaluation_table.reset_index()\n",
    "    dt_list = eval_table['dataset'].unique().tolist()\n",
    "    qtf_list = eval_table['quantifier'].unique().tolist()\n",
    "    arr_table = pd.DataFrame(columns=qtf_list)\n",
    "    alpha = 0\n",
    "    m = len(qtf_list) - 1\n",
    "    for dt in dt_list:\n",
    "        rows_by_dataset = eval_table[eval_table['dataset'] == dt]\n",
    "        arr_row = []\n",
    "        for qtf in qtf_list:\n",
    "            acc_i = np.array(rows_by_dataset[rows_by_dataset['quantifier'] == qtf]['inv_abs_error'].values)\n",
    "            acc_j = np.array(rows_by_dataset[rows_by_dataset['quantifier'] != qtf]['inv_abs_error'].values)\n",
    "\n",
    "            run_time_i = np.array(rows_by_dataset[rows_by_dataset['quantifier'] == qtf]['run_time'].values)\n",
    "            run_time_j = np.array(rows_by_dataset[rows_by_dataset['quantifier'] != qtf]['run_time'].values)\n",
    "\n",
    "            acc_i_div_j = acc_i / acc_j\n",
    "            run_time_i_div_j = 1 + (alpha * np.log10(run_time_i / run_time_j))\n",
    "            arr_i = np.sum(acc_i_div_j / run_time_i_div_j) / m\n",
    "\n",
    "            arr_row.append(arr_i)\n",
    "        arr_table.loc[dt] = arr_row\n",
    "\n",
    "    data = meta_features_table.values\n",
    "    model.fit(data) # NearestNeighbors(n_neighbors=n_neighbors, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "    return model, arr_table\n",
    "\n",
    "\n",
    "\n",
    "def loo_knn(model, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table,  recommender_eval_path: str = None):\n",
    "    # predicted_arr_table = pd.DataFrame(columns=self.arr_table.columns, index=self.arr_table.index.tolist())\n",
    "    # true_arr_table = pd.DataFrame(columns=self.arr_table.columns, index=self.arr_table.index.tolist())\n",
    "\n",
    "    recommender_evaluation_table = pd.DataFrame(columns=[\"predicted_ranking\",\n",
    "                                                            \"predicted_ranking_weights\",\n",
    "                                                            \"predicted_ranking_arr\",\n",
    "                                                            \"true_ranking\",\n",
    "                                                            \"true_ranking_weights\",\n",
    "                                                            \"true_ranking_arr\"], index=arr_table.index.tolist())\n",
    "    transform_pipeline_ = MinMaxScaler() # clone(self.transform_pipeline)\n",
    "    recommender_ = clone(model)\n",
    "    for dataset in arr_table.index.tolist():\n",
    "        X_test = meta_features_table.loc[dataset].values\n",
    "        X_test = np.array(X_test).reshape(1, -1)\n",
    "        y_test = arr_table.loc[dataset].values\n",
    "\n",
    "        X_train = meta_features_table.drop(index=dataset).values\n",
    "        y_train = (arr_table.drop(index=dataset)).values\n",
    "\n",
    "        transform_pipeline_.fit(X_train)\n",
    "        transformed_train = transform_pipeline_.transform(X_train)\n",
    "        recommender_.fit(transformed_train, y_train)\n",
    "\n",
    "        transformed_test = transform_pipeline_.transform(X_test)\n",
    "        distances, indices = recommender_.kneighbors(transformed_test)\n",
    "        distances, indices = distances[0], indices[0]\n",
    "        quantifiers = arr_table.columns\n",
    "        new_arr_array = np.array(len(quantifiers) * [np.float64(0)])\n",
    "        tolerance = 1e-10\n",
    "        weights = np.array(1/(distances + tolerance)) / np.sum(1/(distances + tolerance))\n",
    "        for idx, w in zip(indices, weights):\n",
    "            arr_idx = meta_features_table.iloc[idx].name\n",
    "            new_arr_array += np.array(arr_table.loc[arr_idx].values) * w\n",
    "\n",
    "        quantifier_arr_pairs = sorted(list(zip(quantifiers, new_arr_array)), key=lambda x: x[1], reverse=True)\n",
    "        predicted_ranking, predicted_arr = zip(*quantifier_arr_pairs)\n",
    "        predicted_ranking_weights = np.array(predicted_arr) / np.sum(predicted_arr)\n",
    "\n",
    "        quantifier_arr_pairs = sorted(list(zip(quantifiers, y_test)), key=lambda x: x[1], reverse=True)\n",
    "        true_ranking, true_arr = zip(*quantifier_arr_pairs)\n",
    "        true_ranking_weights = np.array(true_arr) / np.sum(true_arr)\n",
    "\n",
    "        recommender_evaluation_table.loc[dataset] = [predicted_ranking, predicted_ranking_weights, predicted_arr,\n",
    "                                                        true_ranking, true_ranking_weights, true_arr]\n",
    "        \n",
    "    if not recommender_eval_path is None:\n",
    "        recommender_evaluation_table.to_csv(recommender_eval_path)\n",
    "        \n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table.copy(deep=True)\n",
    "    not_agg_evaluation_table.sort_values(by=['quantifier', 'dataset'], inplace=True)\n",
    "    not_agg_evaluation_table.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return recommender_evaluation_table, not_agg_evaluation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class ARRKNN(BaseEstimator):\n",
    "    def __init__(self, n_neighbors=5, metric=\"minkowski\", p=2, algorithm=\"auto\", tolerance=1e-10):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.algorithm = algorithm\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # y: (n_samples, n_quantifiers)  -> ARR vetor por dataset\n",
    "        self._y_train = np.asarray(y)\n",
    "\n",
    "        self._nn = NearestNeighbors(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            metric=self.metric,\n",
    "            p=self.p,\n",
    "            algorithm=self.algorithm\n",
    "        )\n",
    "        self._nn.fit(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self._nn.kneighbors(X)\n",
    "\n",
    "        preds = []\n",
    "        for dists, idxs in zip(distances, indices):\n",
    "            w = 1.0 / (dists + self.tolerance)\n",
    "            w = w / np.sum(w)\n",
    "            # média ponderada dos vetores ARR dos vizinhos\n",
    "            pred_vec = np.sum(self._y_train[idxs] * w[:, None], axis=0)\n",
    "            preds.append(pred_vec)\n",
    "\n",
    "        return np.asarray(preds)  # (n_samples, n_quantifiers)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "def loo_grid_knn(model, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table,\n",
    "                 param_grid, recommender_eval_path: str = None):\n",
    "    recommender_evaluation_table = pd.DataFrame(\n",
    "        columns=[\"predicted_ranking\",\n",
    "                 \"predicted_ranking_weights\",\n",
    "                 \"predicted_ranking_arr\",\n",
    "                 \"true_ranking\",\n",
    "                 \"true_ranking_weights\",\n",
    "                 \"true_ranking_arr\"],\n",
    "        index=arr_table.index.tolist()\n",
    "    )\n",
    "\n",
    "    for dataset in arr_table.index.tolist():\n",
    "        X_test = meta_features_table.loc[dataset].values.reshape(1, -1)\n",
    "        y_test = arr_table.loc[dataset].values  # vetor ARR verdadeiro\n",
    "\n",
    "        X_train = meta_features_table.drop(index=dataset).values\n",
    "        y_train = arr_table.drop(index=dataset).values  # matriz (99, n_quantifiers)\n",
    "\n",
    "        # mantém seu scaling (fit só no treino do fold externo)\n",
    "        transform_pipeline_ = MinMaxScaler()\n",
    "        transform_pipeline_.fit(X_train)\n",
    "        transformed_train = transform_pipeline_.transform(X_train)\n",
    "        transformed_test = transform_pipeline_.transform(X_test)\n",
    "\n",
    "        # nested tuning interno\n",
    "        inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            estimator=clone(model),\n",
    "            param_grid=param_grid,\n",
    "            scoring=\"neg_mean_absolute_error\",  # funciona com multioutput: agrega MAE nos outputs\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            refit=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        gs.fit(transformed_train, y_train)\n",
    "\n",
    "        # previsão do vetor ARR (igual ao seu new_arr_array)\n",
    "        new_arr_array = gs.best_estimator_.predict(transformed_test)[0]\n",
    "\n",
    "        quantifiers = arr_table.columns\n",
    "\n",
    "        quantifier_arr_pairs = sorted(zip(quantifiers, new_arr_array), key=lambda x: x[1], reverse=True)\n",
    "        predicted_ranking, predicted_arr = zip(*quantifier_arr_pairs)\n",
    "        predicted_ranking_weights = np.array(predicted_arr) / np.sum(predicted_arr)\n",
    "\n",
    "        quantifier_arr_pairs = sorted(zip(quantifiers, y_test), key=lambda x: x[1], reverse=True)\n",
    "        true_ranking, true_arr = zip(*quantifier_arr_pairs)\n",
    "        true_ranking_weights = np.array(true_arr) / np.sum(true_arr)\n",
    "\n",
    "        recommender_evaluation_table.loc[dataset] = [\n",
    "            predicted_ranking, predicted_ranking_weights, predicted_arr,\n",
    "            true_ranking, true_ranking_weights, true_arr\n",
    "        ]\n",
    "\n",
    "    if recommender_eval_path is not None:\n",
    "        recommender_evaluation_table.to_csv(recommender_eval_path)\n",
    "\n",
    "    not_agg_evaluation_table = not_aggregated_evaluation_table.copy(deep=True)\n",
    "    not_agg_evaluation_table.sort_values(by=[\"quantifier\", \"dataset\"], inplace=True)\n",
    "    not_agg_evaluation_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return recommender_evaluation_table, not_agg_evaluation_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb319be",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=1, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "meta_features_table = meta_features\n",
    "evaluation_table = knn_quantifiers_eval_agg\n",
    "not_aggregated_evaluation_table = knn_quantifiers_eval\n",
    "\n",
    "knn, arr_table = fit_knn(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table)\n",
    "\n",
    "knn_recommender_eval, knn_quantifiers_eval = loo_knn(knn, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"knn_1_recommendation_eval.csv\")\n",
    "ensemble_qtf = EnsembleQuantifier()\n",
    "ensemble_qtf.evaluation(\"knn\", knn_recommender_eval, knn_quantifiers_eval, f\"./plot_data/experiment-1/results/knn_1/knn_1_ensemble_quantifier_evaluation_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=3, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "meta_features_table = meta_features\n",
    "evaluation_table = knn_quantifiers_eval_agg\n",
    "not_aggregated_evaluation_table = knn_quantifiers_eval\n",
    "\n",
    "knn, arr_table = fit_knn(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table)\n",
    "\n",
    "knn_recommender_eval, knn_quantifiers_eval = loo_knn(knn, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"knn_3_recommendation_eval.csv\")\n",
    "ensemble_qtf = EnsembleQuantifier()\n",
    "ensemble_qtf.evaluation(\"knn\", knn_recommender_eval, knn_quantifiers_eval, f\"./plot_data/experiment-1/results/knn_3/knn_3_ensemble_quantifier_evaluation_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c51e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=5, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "meta_features_table = meta_features\n",
    "evaluation_table = knn_quantifiers_eval_agg\n",
    "not_aggregated_evaluation_table = knn_quantifiers_eval\n",
    "\n",
    "knn, arr_table = fit_knn(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table)\n",
    "\n",
    "knn_recommender_eval, knn_quantifiers_eval = loo_knn(knn, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"knn_5_recommendation_eval.csv\")\n",
    "ensemble_qtf = EnsembleQuantifier()\n",
    "ensemble_qtf.evaluation(\"knn\", knn_recommender_eval, knn_quantifiers_eval, f\"./plot_data/experiment-1/results/knn_5/knn_5_ensemble_quantifier_evaluation_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c46e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=7, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "meta_features_table = meta_features\n",
    "evaluation_table = knn_quantifiers_eval_agg\n",
    "not_aggregated_evaluation_table = knn_quantifiers_eval\n",
    "\n",
    "knn, arr_table = fit_knn(model, meta_features_table, not_aggregated_evaluation_table, evaluation_table)\n",
    "\n",
    "knn_recommender_eval, knn_quantifiers_eval = loo_knn(knn, arr_table, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"knn_7_recommendation_eval.csv\")\n",
    "ensemble_qtf = EnsembleQuantifier()\n",
    "ensemble_qtf.evaluation(\"knn\", knn_recommender_eval, knn_quantifiers_eval, f\"./plot_data/experiment-1/results/knn_7/knn_7_ensemble_quantifier_evaluation_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d9457",
   "metadata": {},
   "source": [
    "# Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "    model = KNeighborsRegressor()\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_KNN_recommender_eval, REG_KNN_quantifiers_eval = loo_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"neigh_regressor_recommendation_eval.csv\")\n",
    "\n",
    "    REG_KNN_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_recommender_eval.csv\")\n",
    "    REG_KNN_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_quantifiers_eval.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation(\"regression\", REG_KNN_recommender_eval, REG_KNN_quantifiers_eval, f\"./plot_data/experiment-1/results/neigh_reg/reg_ensemble_quantifier_evaluation_table_KNN.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    model=RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_RF_recommender_eval, REG_RF_quantifiers_eval = loo_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"rf_regressor_recommendation_eval.csv\")\n",
    "\n",
    "    REG_RF_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_recommender_eval.csv\")\n",
    "    REG_RF_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_quantifiers_eval.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation(\"regression\", REG_RF_recommender_eval, REG_RF_quantifiers_eval, f\"./plot_data/experiment-1/results/rf_reg/reg_ensemble_quantifier_evaluation_table_RF.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55072ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    model=XGBRegressor(n_jobs=-1)\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_XGBR_recommender_eval, REG_XGBR_quantifiers_eval = loo_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"xgbr_regressor_recommendation_eval.csv\")\n",
    "\n",
    "    REG_XGBR_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/xgbr_reg/xgbr_reg_recommender_eval.csv\")\n",
    "    REG_XGBR_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/xgbr_reg/xgbr_reg_quantifiers_eval.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation(\"regression\", REG_XGBR_recommender_eval, REG_XGBR_quantifiers_eval, f\"./plot_data/experiment-1/results/xgbr_reg/reg_ensemble_quantifier_evaluation_table_XGBR.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda15ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.svm import SVR\n",
    "\n",
    "    model=SVR()\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_SVR_recommender_eval, REG_SVR_quantifiers_eval = loo_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, RESULTS_DIR + \"svr_regressor_recommendation_eval.csv\")\n",
    "\n",
    "    REG_SVR_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_recommender_eval.csv\")\n",
    "    REG_SVR_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_quantifiers_eval.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation(\"regression\", REG_SVR_recommender_eval, REG_SVR_quantifiers_eval, f\"./plot_data/experiment-1/results/svr_reg/reg_ensemble_quantifier_evaluation_table_SVR.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffc723",
   "metadata": {},
   "source": [
    "# Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNNING\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "try:\n",
    "    model=KNeighborsRegressor()\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    # param_grid = {\n",
    "    #     \"n_neighbors\": [1, 3, 5, 7, 9, 15, 25, 35],\n",
    "    #     \"weights\": [\"uniform\", \"distance\"],\n",
    "    #     \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    #     \"p\": [1, 2],\n",
    "    #     \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    # }\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": Integer(1, 50),\n",
    "        \"weights\": Categorical([\"uniform\", \"distance\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_NEIGH_recommender_eval, REG_NEIGH_quantifiers_eval = loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, RESULTS_DIR + \"neigh_regressor_recommendation_eval_GRID.csv\")\n",
    "\n",
    "    REG_NEIGH_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_recommender_eval_GRID.csv\")\n",
    "    REG_NEIGH_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_quantifiers_eval_GRID.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation_opt(\"regression\", REG_NEIGH_recommender_eval, REG_NEIGH_quantifiers_eval, f\"./plot_data/experiment-1/results/neigh_reg/reg_ensemble_quantifier_evaluation_table_KNN_GRID.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNNING\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "try:\n",
    "    model=RandomForestRegressor()\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    # param_grid = {\n",
    "    #     \"n_estimators\": [100, 300, 500, 800],\n",
    "    #     \"max_depth\": [None, 5, 10, 20, 40],\n",
    "    #     \"min_samples_split\": [2, 5, 10, 20],\n",
    "    #     \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    #     \"max_features\": [\"sqrt\", \"log2\", 0.3, 0.5, 0.8],\n",
    "    #     \"bootstrap\": [True, False],\n",
    "    # }\n",
    "    param_grid = {\n",
    "        \"n_estimators\": Integer(50, 600),\n",
    "        \"max_depth\": Integer(2, 50),          # se quiser permitir None, dá pra tratar à parte\n",
    "        \"min_samples_leaf\": Integer(1, 20),\n",
    "    }\n",
    "\n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_RF_recommender_eval, REG_RF_quantifiers_eval = loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, RESULTS_DIR + \"rf_regressor_recommendation_eval_GRID.csv\")\n",
    "\n",
    "    REG_RF_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_recommender_eval_GRID.csv\")\n",
    "    REG_RF_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_quantifiers_eval_GRID.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation_opt(\"regression\", REG_RF_recommender_eval, REG_RF_quantifiers_eval, f\"./plot_data/experiment-1/results/rf_reg/reg_ensemble_quantifier_evaluation_table_RF_GRID.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNNING\n",
    "from ensemble_quantifier import EnsembleQuantifier\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "try:\n",
    "    from sklearn.svm import SVR\n",
    "\n",
    "    model=SVR()\n",
    "\n",
    "    meta_features_table = meta_features\n",
    "    evaluation_table = reg_quantifiers_eval_agg\n",
    "    not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "    # param_grid = {\n",
    "    #     \"kernel\": [\"rbf\"],\n",
    "    #     \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    #     \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],\n",
    "    #     \"epsilon\": [0.001, 0.01, 0.1, 0.5, 1],\n",
    "    # }\n",
    "    param_grid = {\n",
    "        \"kernel\": Categorical([\"rbf\", \"linear\"]),   # se quiser só RBF: [\"rbf\"]\n",
    "        \"C\": Real(1e-3, 1e3, prior=\"log-uniform\"),\n",
    "        \"gamma\": Real(1e-4, 1e0, prior=\"log-uniform\"),\n",
    "        \"epsilon\": Real(1e-4, 1e0, prior=\"log-uniform\"),\n",
    "    }\n",
    "    \n",
    "\n",
    "    model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "    REG_SVR_recommender_eval, REG_SVR_quantifiers_eval = loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, RESULTS_DIR + \"svr_regressor_recommendation_eval_GRID.csv\")\n",
    "\n",
    "    REG_SVR_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_recommender_eval_GRID.csv\")\n",
    "    REG_SVR_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_quantifiers_eval_GRID.csv\")\n",
    "\n",
    "    ensemble_qtf = EnsembleQuantifier()\n",
    "    ensemble_qtf.evaluation(\"regression\", REG_SVR_recommender_eval, REG_SVR_quantifiers_eval, f\"./plot_data/experiment-1/results/svr_reg/reg_ensemble_quantifier_evaluation_table_SVR_GRID.csv\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1559b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FINE-TUNNING\n",
    "# from ensemble_quantifier import EnsembleQuantifier\n",
    "# try:\n",
    "#     from sklearn.svm import SVR\n",
    "\n",
    "#     model=SVR()\n",
    "\n",
    "#     meta_features_table = meta_features\n",
    "#     evaluation_table = reg_quantifiers_eval_agg\n",
    "#     not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "#     param_grid = {\n",
    "#         \"kernel\": ['linear'],\n",
    "#         \"C\": [0.01, 0.1, 1, 10, 100, 1000],\n",
    "#         \"epsilon\": [0.001, 0.01, 0.1, 0.5, 1],\n",
    "#     }\n",
    "\n",
    "#     model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "#     REG_SVR_recommender_eval, REG_SVR_quantifiers_eval = loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, RESULTS_DIR + \"svr_regressor_recommendation_eval_GRID_2.csv\")\n",
    "\n",
    "#     REG_SVR_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_recommender_eval_GRID_2.csv\")\n",
    "#     REG_SVR_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_quantifiers_eval_GRID_2.csv\")\n",
    "\n",
    "#     ensemble_qtf = EnsembleQuantifier()\n",
    "#     ensemble_qtf.evaluation(\"regression\", REG_SVR_recommender_eval, REG_SVR_quantifiers_eval, f\"./plot_data/experiment-1/results/svr_reg/reg_ensemble_quantifier_evaluation_table_SVR_GRID_2.csv\")\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FINE-TUNNING\n",
    "# from ensemble_quantifier import EnsembleQuantifier\n",
    "# try:\n",
    "#     from sklearn.svm import SVR\n",
    "\n",
    "#     model=SVR()\n",
    "\n",
    "#     meta_features_table = meta_features\n",
    "#     evaluation_table = reg_quantifiers_eval_agg\n",
    "#     not_aggregated_evaluation_table = reg_quantifiers_eval\n",
    "\n",
    "#     param_grid = {\n",
    "#         \"kernel\": [\"poly\"],\n",
    "#         \"C\": [0.1, 1, 10, 100],\n",
    "#         \"degree\": [2, 3, 4],\n",
    "#         \"gamma\": [\"scale\", 0.01, 0.1],\n",
    "#         \"coef0\": [0, 0.1, 1],\n",
    "#         \"epsilon\": [0.01, 0.1, 0.5]\n",
    "#     }\n",
    "\n",
    "#     model_dict = fit_regressor(model=model, meta_features_table=meta_features_table, not_aggregated_evaluation_table=not_aggregated_evaluation_table, evaluation_table=evaluation_table)\n",
    "#     REG_SVR_recommender_eval, REG_SVR_quantifiers_eval = loo_grid_reg(model_dict, meta_features_table, not_aggregated_evaluation_table, evaluation_table, param_grid, RESULTS_DIR + \"svr_regressor_recommendation_eval_GRID_3.csv\")\n",
    "\n",
    "#     REG_SVR_recommender_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_recommender_eval_GRID_3.csv\")\n",
    "#     REG_SVR_quantifiers_eval.to_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_quantifiers_eval_GRID_3.csv\")\n",
    "\n",
    "#     ensemble_qtf = EnsembleQuantifier()\n",
    "#     ensemble_qtf.evaluation(\"regression\", REG_SVR_recommender_eval, REG_SVR_quantifiers_eval, f\"./plot_data/experiment-1/results/svr_reg/reg_ensemble_quantifier_evaluation_table_SVR_GRID_3.csv\")\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from ensemble_quantifier import EnsembleQuantifier\n",
    "\n",
    "# ## REGRESSORES\n",
    "\n",
    "# # KNN\n",
    "# REG_KNN_recommender_eval = pd.read_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_recommender_eval.csv\", index_col=0)\n",
    "# REG_KNN_quantifiers_eval = pd.read_csv(f\"./plot_data/experiment-1/results/neigh_reg/neigh_reg_quantifiers_eval.csv\", index_col=0)\n",
    "# ensemble_qtf = EnsembleQuantifier()\n",
    "# ensemble_qtf.evaluation(\"regression\", REG_KNN_recommender_eval, REG_KNN_quantifiers_eval, f\"./plot_data/experiment-1/results/neigh_reg/reg_ensemble_quantifier_evaluation_table_KNN.csv\")\n",
    "\n",
    "# # RF\n",
    "# REG_RF_recommender_eval = pd.read_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_recommender_eval.csv\", index_col=0)\n",
    "# REG_RF_quantifiers_eval = pd.read_csv(f\"./plot_data/experiment-1/results/rf_reg/rf_reg_quantifiers_eval.csv\", index_col=0)\n",
    "# ensemble_qtf = EnsembleQuantifier()\n",
    "# ensemble_qtf.evaluation(\"regression\", REG_RF_recommender_eval, REG_RF_quantifiers_eval, f\"./plot_data/experiment-1/results/rf_reg/reg_ensemble_quantifier_evaluation_table_RF.csv\")\n",
    "\n",
    "# # XGBOOST\n",
    "# REG_XGBR_recommender_eval = pd.read_csv(f\"./plot_data/experiment-1/results/xgbr_reg/xgbr_reg_recommender_eval.csv\", index_col=0)\n",
    "# REG_XGBR_quantifiers_eval = pd.read_csv(f\"./plot_data/experiment-1/results/xgbr_reg/xgbr_reg_quantifiers_eval.csv\", index_col=0)\n",
    "# ensemble_qtf = EnsembleQuantifier()\n",
    "# ensemble_qtf.evaluation(\"regression\", REG_XGBR_recommender_eval, REG_XGBR_quantifiers_eval, f\"./plot_data/experiment-1/results/xgbr_reg/reg_ensemble_quantifier_evaluation_table_XGBR.csv\")\n",
    "\n",
    "# # SVR\n",
    "# REG_SVR_recommender_eval = pd.read_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_recommender_eval.csv\", index_col=0)\n",
    "# REG_SVR_quantifiers_eval = pd.read_csv(f\"./plot_data/experiment-1/results/svr_reg/svr_reg_quantifiers_eval.csv\", index_col=0)\n",
    "# ensemble_qtf = EnsembleQuantifier()\n",
    "# ensemble_qtf.evaluation(\"regression\", REG_SVR_recommender_eval, REG_SVR_quantifiers_eval, f\"./plot_data/experiment-1/results/svr_reg/reg_ensemble_quantifier_evaluation_table_SVR.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
